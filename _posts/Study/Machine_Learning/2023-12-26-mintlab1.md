---
title: "[MINT LAB] Multi-head Attention"
excerpt: ""
categories: [MachineLearning]
tags: [ML, ViT]
toc: true
toc_sticky: true
---

병렬로 일 수행

* 목적: 한 헤드의 소프트맥스가 유사도의 한 측면에만 초점을 맞추는 경향이 있음, 입력 벡터가 굉장히 크면 softmax 과정에서 큰 값들이 반영이 잘 안됨 -> 따라서 각 구간을 나누어 각각을 softmax. 모델이 여러 측면에 초점을 맞추게 하기 위함


내가 이해를 하는거
* 하나의 attention에는 q, k, v가 있음
* head_dim은 차원임
* 각각의 어텐션 헤드의 출력을 연결하면 멀티 어텐션 헤드가 됨
* MultiheadAttention 클래스에서 AttentionHead 클래스 호출


### ChatGPT says...
MultiHeadedAttention 클래스는 입력 데이터의 다양한 특성과 관계를 포착하고, 이를 통해 더 정교한 표현을 학습할 수 있습니다. 이는 트랜스포머 모델이 자연어 처리(NLP), 이미지 인식, 그리고 기타 여러 분야에서 뛰어난 성능을 발휘하는 데 핵심적인 역할을 합니다.

1. 입력 데이터의 변환: 입력 데이터는 세 개의 밀집(Dense) 층을 거쳐 쿼리(query), 키(key), 값(value)으로 변환됩니다. 이 변환은 입력 데이터의 다양한 표현을 생성하여, 어텐션 메커니즘에 사용됩니다.
2. 멀티 헤드 분할: 변환된 쿼리, 키, 값은 멀티 헤드 어텐션을 위해 여러 헤드로 분할됩니다. 이 분할을 통해 모델은 데이터의 다양한 부분에 동시에 주의를 기울일 수 있습니다. 각 헤드는 입력 데이터의 서로 다른 특성에 집중하여, 전체적인 이해도를 높입니다.
3. 스케일드 닷 프로덕트 어텐션: 각 헤드는 스케일드 닷 프로덕트 어텐션을 수행합니다. 이 과정에서 쿼리와 키의 점곱을 계산하고, 이를 스케일링하여 어텐션 점수를 생성합니다. 이 점수는 소프트맥스 함수를 통해 확률 분포로 변환되며, 최종적으로 값에 가중치를 적용합니다.
4. 결과의 병합 및 변환: 각 헤드에서 생성된 어텐션 결과는 다시 하나로 병합되고, 최종 밀집 층을 통과하여 출력됩니다. 이 과정은 멀티 헤드 어텐션의 결과를 통합하여, 입력 데이터에 대한 풍부한 표현을 생성합니다.

***

## ViT
* 이미지 패치화: 입력 이미지를 고정 크기의 작은 정사각형 패치로 나눔. 일반적으로 224x224 크기로 전처리되며, 그렇게 되면 16x16크기의 패치로 나눔. 이 잘라진 패치들을 신경망에 입력
* CNN보다 메모리를 많이 씀. 큰 모델임