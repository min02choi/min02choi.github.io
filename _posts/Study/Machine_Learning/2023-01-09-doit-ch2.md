---
title: "[Doit! 딥러닝 교과서] Ch2. 순방향 신경망"
excerpt: "순방향 신경망의 구조와 설계 항목"
categories: [MachineLearning]
tags: [ML, DL]
toc: true
toc_sticky: true
---

# Ch2.1 순방향 신경망의 구조와 설계 항목

## 2.1.1 순방향 신경망의 구조
### 순방향 신경망
데이터가 한 방향으로 전달되는 순방향 연결만을 갖는 구조
* 뉴런들이 모여 계층을 이루고 계층이 쌓여 전체 신경망을 이룸
  * 입력 계층: 외부에서 데이터를 전달받음
  * 은닉 계층: 데이터의 특징 추출 -> 문제의 복잡도에 따라 가변적으로 구성
  * 출력 계층: 출력된 특징을 기반으로 추론한 결과를 외부에 출력
* 모든 계층이 완전 연결 계층
  * 이전 계층에서 출력한 데이터를 동일하게 전달받음 -> 각 뉴런마다 서로 다른 측징 추출

[가중 합산] <br/>
추출할 특징에 중요한 영향을 미치는 데이터를 선택하는 과정
> z = w1x1 + w2x2 + ... +wnxn + b <br/> 
> = w^Tx + b
* 입력 데이터(x)가 들어오면 가중치(w)와 곱해서 가중 합산을 함
* 가중치: 특징을 추출할 때 영향이 큰 데이터를 선택하는 역할
* b로 오프셋 지정

[활성 함수] <br/>
원하는 형태로 특징을 추출하기 위해 데이터를 비선형 적으로 변환하는 과정
* ReLU(Rectified Liner Unit)
  * 기본 활성 함수
  * 경첩 형태의 비선형 함수


## 범용 함수 근사기로서의 신경망
### 뉴런
가중합산과 활성함수를 순차 실행하는 합성함수 => 실함수로 정의
* 가중 합산한 결과를 비선형 활성 함수로 매핑해서 실수 출력

### 계층
뉴런의 그룹으로 정의 -> 합성함수 => 벡터 함수로 정의
* 입력 벡터의 크기: 이전 계층의 뉴런 수, 출력 벡터의 크기: 현재 계층의 뉴런 수
* 계층의 가중치는 크기가 n * m인 행렬 W로 정의
* 가중 합산 결과에 활성 함수 실행

### 신경망
계층을 순차적으로 쌓은 형태 -> 합성함수 => 벡터 함수로 정의
* 각 계층이 정의하는 벡터 함수를 순차적으로 실행
* 입력 x를 출력 y로 매핑하는 y = f(x; 파이) 형태의 함수
  * 파이: 뉴런의 가중치와 편향을 포함한 함수의 파라미터
* 신경망은 학습할 때 미분 사용 -> 신경망이 표현하는 함수는 미분가능한 함수이어야 함
* 범용 함수 근사기, 확장성이 뛰어난 모델 구조

> 실함수와 벡터 함수
> * 실함수: Rn -> R
>   * 입력: 크기가 n인 벡터, 출력: 실수
> * 벡터함수: Rn -> Rm
>   * 입력: 크기가 n인 벡터, 출력: 크기가 m인 벡터

### 범용 근사 정리
범용 근사기: 신경망은 n차원 공간의 임의의 연속함수를 근사하는 능력이 있음
* 범용 근사 정리로 증명
  * 범용 근사 정리: 2계층의 순방향 신경망에서 은닉 뉴런을 충분히 사용하고 검증된 활성 함수를 사용하면 n차원 공간의 임의의 연속 함수를 원하는 정도의 정확도로 근사할 수 있음
  * 문제: 족잡한 문제를 정확히 근사하려면 은닉 뉴런 수 증가 -> 모델이 커짐 -> 과적합 발생, 모델 성능 저하
  * 문제 해결: 깊은 신경망으로 확장

## 2.1.3 순방향 신경망의 설계 항목
### 순방향 신경망 고려 항목
1. 모델의 입력
2. 출력 형태
3. 활성 함수의 종류
4. 네크워크 크기

출력/입력 형태는 모델이 정해지면 어느정도 결정되므로 중요한 것은 모델의 크기와 활성 함수의 종류를 결정하는 것