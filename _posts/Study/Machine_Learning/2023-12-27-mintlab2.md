---
title: "[MINT LAB] Vision Transformer"
excerpt: "개념공부"
categories: [MachineLearning]
tags: [ML, ViT]
toc: true
toc_sticky: true
---

> [Vision Transformer] An Image is Worth 16 x 16 Words : Transformer for Image Recognition at Scale

# TRANSFORMER : Attention Is All You Need(2017)

자연어 처리

## RNN
Many to Many
* 워드 임베딩: 단어를 벡터로 바꾸는 것

## LSTM
* 단점: 벡터의 크기가 고정되어 있어야 함(Seq2Seq)

## GRU
LSTM이 하나의 셀에 4개의 셀이 있어 컴퓨팅적으로 속도가 느린 단점이 있음
따라서 신경망이 3개인 GRU로 발전

## TRANSFORMER : Attention Is All You Need(2017)
* Attention: 입력 시퀀스 전체에서 정보를 추출