---
title: "[MINT LAB] Vision Transformer"
excerpt: "개념공부"
categories: [MachineLearning]
tags: [ML, ViT]
toc: true
toc_sticky: true
---

> [Vision Transformer] An Image is Worth 16 x 16 Words : Transformer for Image Recognition at Scale

# TRANSFORMER : Attention Is All You Need(2017)

자연어 처리

## RNN
Many to Many
* 워드 임베딩: 단어를 벡터로 바꾸는 것
* 이어서 학습을 하게 되니 back propagation 과정에서 vanishing 문제가 발생함. 이 문제를 해결환 것이 LSTM

## LSTM
* 단점: 벡터의 크기가 고정되어 있어야 함(Seq2Seq)

## GRU
LSTM이 하나의 셀에 4개의 셀이 있어 컴퓨팅적으로 속도가 느린 단점이 있음
따라서 신경망이 3개인 GRU로 발전

***

## TRANSFORMER : Attention Is All You Need(2017)
* Attention: 입력 시퀀스 전체에서 정보를 추출

* Transformer: 독일어 인풋이 들어오면 아웃풋으로 영어를 번역해 주는 것이 transformer임. 내부 구조는 크게 인코더와 디코더로 나뉨
* 인코더 블록에는 self-attention과 뉴럴 네트워크로 구성이 되어 있음
* 디코더에서는 asked self-attention과 인코더 디코더 self attention, 뉴럴 네트워크로 구성이 되어있음
* 전체 백터의 차원은 512임. 인코더 블록-> 문장끝에 end 를 나타내는 토큰을 넣고 나머지 공간에는 padding을 넣음 


**단어 처리의 경우**
1. 단어들을 백터로 바꾸는 워드 임베딩 과정: 
   * 각각의 단어를 하나의 벡터로 바꾸고, 여기에 위치 정보까지 나타내는 벡터(positioning encoding vector)를 더하여 생성

2. Self-Attention에 넣음
   * 각각의 query weight, key weight, value weight 을 구함
   * [query key를 내적해서 스코어를 구함] 특정 단어의 query vector을 가지고 나머지 단어의 모든 key와 연산을 해줌 -> 그러면 가장 query와 비슷한 벡터가 큰 값을 가지게 됨(벡터의 내적 사용)(가장 벡터가 비슷한 것은 값이 크게 나옴-> 가장 유사한 단어임을 알 수 있음)
   * [구한것을 정규화(normalization)] 큰 벡터들을 줄이기 위함(래디언트 안정을 위해)
   * [softmax 으로 확률로 변환]
   * softmax 값과 value를 곱함
   * 위의 값을 모두 sum 함. z 벡터가 나오게 됨


**Attention**
1. 세개의 q k v 벡터로 나눠줌
2. q, k의 내적후 차원에 루트를 씌운 값을 나눠줌으로써 정규화
3. 2의 값에 softmax를 취함
4. 3에 value를 곱함(애도 내적 matmul)
5. 4의 값을 전부 더함 -> z 벡터가 나옴

위의 과정을 여러번 수행한 것이 multi-head attention
* 각각의 워드 임베딩마다 각각의 아웃풋이 나오게 됨(하나의 단어마다 하나의 z가 나오게 됨)

**Multi-Head Attention**
Attention Head가 8개라고 가정
1. 하나의 임베딩된 단어를 각각의 8개의 어텐션에 넣음(?) 
2. 각각의 Attention에서 나온 값을 concat 해줌


디코딩
학습데이터 뒤에 0 (-inf)을 넣어주는 이유로는 학습하는 과정에서 뒤를 보지 말라고 cheating을 방지하기 위함 


* 인코더에서 디코더로 k, v를 넘겨줌


3 Attention
1. Encoder Self-Attention: 각각의 어떤 연관성을 가지는지
2. Masked Decoder Self-Attention: 뒤의 치팅 방지. 앞에거로만 함
3. Encoder-Decoder Attention: 디코더의 쿼리가 인코더의 어떤 벡터에 더 많은 가중치를 두는지 계산하는 과정


***

## [Vision Transformer] An Image is Worth 16 x 16 Words : Transformer for Image Recognition at Scale

Vision Transformer의 목적: 이미지 classification
* input에 새로운 이미지가 들어왔을 때 이 이미지가 어떤 이미지인지 구별해 내는 것

1. 이미지(48/*48)사이즈를 16/*16으로 해서 총 9패치. 이 패치를 단어처럼 생각! 각각의 패치들을 트렌스포머 인코딩에 넣어줌. 근데 여기에서 사진도 순서가 중요하므로 순서정보까지 추가함. 가장 앞부분에 position embedding 까지 넣어줌
2. 각각을 임베딩해 넣어주고 각각을 normalization 을 취해줌
3. 이를 모두 합쳐줌
4. Self-Attention 구조에 들어가기 위해 해당 벡터에 weight를 곱하여 q k v로 나누어줌
5. 이를 Multi-Head Attention 구조로 반복해줌
6. 통과한 아웃풋과 통과하지 않은 아웃풋을 더해서 기존 값을 보존해 줌
7. 이를 normalization 해준 뒤 Multi Layer Perceptron (MLP) 를 통과. 통과한 값과 기존의 값을 더함으로써 최종 output 를 도출함
8. Transformer Encoder 의 아웃풋은 MLP를 통하여 어떤 이미지인지 최종적으로 알 수 있음