---
title: "09. RNN, LSTM, GRU"
excerpt: "testing posting RA"
categories: [DataPatternRecognition]
tags: [Model]
toc: true
toc_sticky: true
---


머신러닝 기반 시계열 데이터 분석 방법론
* 선형 회귀, 로지스틱 회귀
* SVM
* 랜덤 포레스트

인공지능 기반 시계열 데이터 분석 방법론 
* RNN
* LSTM
* GRU
* Seq2Seq

> 뉴럴 네트워크 모델(DNN)
> * 설명변수(x), 은닉층(h), 반응변수(y)
> * ht = f(Wxh * Xt)
> * 이전 시점 정보를 반영하지 않음

## RNN(Recurrent Neural Networks)
뉴럴 네트워크의 한계(: 이전 시점을 반영하지 않음)를 극복
* ht = f(Wxh * Xt) + 이전 시점 정보

### 순환신경망 구조 다양성
순차적인 입력의 길이, 순차적인 예측의 길이에 따라 다음과 같이 구분
* Many to One: 여러시점의 다변량 데이터 주어졌을 때 특정 시점 예측
* One to Many: 이미지 데이터 주어졌을 때 정보를 글로 생성
* Many to Many
  * 문장이 주어졌을 때, 각 단어의 품사 예즉(Pos Tagging)(M:M)
  * 영어 문장이 주어졌을 때, 한글문장으로 번역(M:N)

### 순환신경망 학습
* t 시점까지의 과거 정보를 활용하여 yt 예측
* 학습 파라미터 3개
  * Wxh: t시점 데이터 반영
  * Whh: t시점 이전 정보 반영
  * Why: t시점 y를 예측
* 해당 파라미터는 매 시점마다 공유하는 구조
* 매 시점 파라미터를 구성하는 값이 같음
* 최적의 W구하기 목표. 최적의 W는 Loss를 최소 만드는 값

1. Loss 계산하기: Foward Propagation
   * 특정 W일때의 Loss는 얼마인가?
2. Gradient 계산하기: Backward Propagation
   * W일때 Loss에 얼마나 영향을 미치는가?
   * BPTT(오차 역전파법)
3. Parameter Update(학습)
   * W를 어떻게 더 좋은 수치로 업데이트 할 것인가?: Loss를 줄이는 방법 -> Gradient Decent

### 순환신경망 한계점
* 장기 의존성 문제
* Sequence의 길이가 길어질수록 과거 정보 학습에 어려움 발생
  * 원인: 기울기 소실 문제 -> gradient 값이 chain rule을 통해 0으로 수렴


## LSTM(Long Short Term Memory)
RNN의 장기 의존성 문제를 완화한 RNN개선 모델
* Cell State 구조, 세가지 Gate
  * Cell State: 장기 기억
  * Hidden State: 단기 기억
  * Forget Gate: 과거정보 수용 게이트
  * Input Gate: 현재 데이터 수용 정도 게이트
  * Output Gate: Hidden State에 Cell State를 얼마나 반영할지 결정하는 게이트

### LSTM 구조
1. Gate 계산하기
   * 얼마나 적용할 것인지에 대한 가중치 역할
   * Gate마다 서로 다른 파라미터를 지님
2. Cell State 업데이트 하기
   * 임시 Cell State 생성, 탄젠트 함수로 -1~1 사이 벡터로 산출
   * 임시 Cell State: 현재 입력값과 과거 Hidden State 정보에 대한 정보 요약
3. Hidden State 업데이트 하기


## GRU(Gated Recurrent Unit)
LSTM 구조 개선 모델
* Gate 수 감소
  * forget gate, input gate -> update gate(z)
  * output gate -> reset gate(r)
* 이전 cell state 이용
  * cell state, hidden state를 hidden state로 통합
