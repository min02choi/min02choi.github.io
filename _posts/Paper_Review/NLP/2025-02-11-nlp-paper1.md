---
title: "Attention Is All You Need"
excerpt: "새로 카테고리 만들어서 옮길 것"
categories: [NLP]
tags: []
toc: true
toc_sticky: true
---

> 기존 연구는 어떻게 진행되어 왔는데(background),  
> 아직 이 부분에 대해서는 너무 부족해(motivaiton).  
> 그래서 우리는 이렇게 해결해보려고(contribution)  
> 이런 방법을 적용했어(method) 그 결과는 이렇고(result)  
> 그 이유는 우리가 제안한 ~ 때문이야(discussion)  

# Abstract
기존의 시퀀스 변환 모델:  
* 복잡한 반복이나 CNN
* attention 메커니즘을 포함한 encoder decoder 구조

그래서 **Transformer**을 제안
* 오로지 attention 메커니즘을 기반으로 함
* 수평적임(parallelizable)
* 훈련에 적은 시간 소요


# 1. Introduction
RNN, CNN, 그리고 최신 기술의 한계:
* 입력, 출력 데이터의 시퀀스를 계산함 --> hidden state의 시퀀스를 만들게 되는데, 시퀀스가 길어지게 되면 수평적이지 않고, 메모리의 한계가 발생함.
* 최근 연구에서는 분해하는 방법(factorization tricks)이나 조건부 연산(conditional computation)을 사용하기도 했는데, 여전히 시퀀스 연산에 대한 제약이 있음.


# 2. Background
Goal: 시퀀스 연산을 줄이고자 함

기존의 모델들 (ByteNet, ConvS2S): 합성곱 연산을 수행함
* 이는 입력과 출력의 데이터 쌍의 거리에 따라 연산의 수가 달라짐 --> 거리가 멀어질수록 학습을 더 어렵게 만들음.
  * ConvS2S: 선형적
  * ByteNet: 로그적

**Transformer**은 이러한 연산수를 상수로 줄임
* 연산의 수를 줄임에 따라서 해상도의 감소가 있지만, 이는 추후 **Multi-head Attention** 기법으로 보완할 것임


# 3. Model Architecture
![Transformer model architecture](image_file/paper1-transformer.png)
* encoder decoder 구조로 되어있음
* stacked self-attention
* point-wise, fully conneted layers

## 3.1 Encoder and Decoder Stacks
### Encoder(N = 6)
* 입력 시퀀스(x1, ..., xn) 를 연속적인 표현으로 매핑 z = (z1, ..., zn)
* 각각의 layers는 두개의 sublayers가 있음
  1. multi-head self-attention mechanism
  2. position-wise fully connected feed-forward network
* 각각의 sublayers를 통과하면 residual connection, layer normalization이 뒤따름

최종 아웃풋  
==> LayerNorm(x + Sublayer(x))
* dimension = 512

### Decoder(N = 6)
* 세 개의 sub-layers
  1. multi-head self-attention mechanism
  2. position-wise fully connected feed-forward network
  3. masked multi-head attention
     * i 번째 prediction에 대해 i 이후의 값을 참고하지 않게 함(masking)
* encoder과 마찬가지로, 각 sublayers에 대해 residual connection, layer normalization이 뒤따름


## 3.2 Attention
![Attention architecture](./image_file/paper1-attention.png)

쿼리를 매핑  
key-value 쌍을 결과로 출력, 결과는 value의 가중합으로 계산됨  
각각의 value에 대한 가중치는 q, k 사이 compatibility function(호환성 함수, 예: dot-product)으로 계산됨 

**핵심 변수**  
쿼리(Query)
* 입력된 특정 데이터 포인트(예: 현재 단어 또는 토큰)를 표현하는 벡터.
* "무엇을 찾고 싶은지"를 나타냄.

키(Keys)
* 데이터 포인트 전체를 나타내는 벡터 집합에서 각 데이터 포인트의 특징을 나타냄.
* "이 데이터 포인트가 중요한지 판단하기 위한 기준" 역할.

값(Values)
* 실제로 반환할 정보(데이터 포인트의 표현)를 담고 있는 벡터.
* 쿼리가 어떤 키에 주목했는지에 따라 가중치를 받아서 출력에 반영됨.

Attention 동작 과정
1. 쿼리가 입력으로 주어짐.
2. 쿼리와 각 키의 호환성을 계산하여 가중치를 생성.
3. 값들(values)에 가중치를 곱해 가중합을 계산.
4. 최종적으로, 이 가중합이 출력으로 반환.

### 3.2.1 Scaled Dot-Product Attention
* q, k의 dim = $d_k$
* v의 dim = $d_v$

1. q, k의 dot product 계산
2. 각각을 $\sqrt{d_k}$ 로 나누고(scaling factor), softmax 적용하여 values의 가중치 계산

> 즉 q, k는 v의 가중치를 구하기 위해 사용되는 값

Attention$(Q, K, V)$ = softmax$(\frac{QK^T}{\sqrt{d_k}})V$

> **주요 Attention 매커니즘**  
> * Additive Attention: q, k를 선형 변환 후, 비선형 활성화 함수를 사용하여 계산
> * Dot-Product Attention: q, k의 점곱 후 scale
>   
> Dot-Product Attention은 계산량이 적고, GPU/TPU에서 병렬 처리하기에 적합하여 현대적 모델에서 주로 사용

Attention 메커니즘은 Dot-Product Attention 채택(faster, space-efficient)

### 3.2.2 Multi-Head Attention
